---
title: "Reanalysis of BELIV Paper Study Data"
author: "Matthew Kay"
date: "July 18, 2016"
output: html_document
---

Based on `simpleviz-reanalysis.Rmd`.

# Setup

## Imports

```{r setup}
library(dplyr)
library(ggplot2)
library(metabayes)
library(tidybayes)
library(rstan)
library(rethinking)
import::from(gamlss, gamlss, random, re)
import::from(gamlss.dist, 
    NO, dNO, pNO, qNO, rNO, 
    TF, dTF, pTF, qTF, rTF)
import::from(gamlss.tr, gen.trun, trun.r)
```

## GGplot theme

```{r theme}
theme_set(theme_light())
```

## Stan setup

```{r stan}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

# Load and clean data

First, let's read in the data:

```{r message=FALSE}
sv <- read.csv("data/simplevis.csv") %>%
    rename(
        p = Value,
        response = Estimate,
        participant = ID,
        vis = Type
    )
head(sv)
```


# Explore first

Let's see the shape of people's error. We will particularly be interested in `squarepie`, which I suspect people are reading essentially as two bar charts (one for the tens column and one for the ones column), but let's start by looking at all of the responses versus the actual probability. We'll include the line `y = x` (black line) and a plain-old linear regression (red) to get a sense of the shape of things:

```{r}
sv %>%
    ggplot(aes(x = p, y=response)) +
    geom_point(alpha = 0.2, size = 1) +
    geom_abline(intercept = 0, slope=1, size = .5) + 
    stat_smooth(method = lm, color="red", se=FALSE) +
    facet_wrap(~vis)
```

Overall, the first impression here is that people's estimates look pretty good in all of the conditions, and that generally speaking there doesn't seem to be any big biases here (if any at all). The top end of some of the conditions *might* show some underestimation, but as I will argue later, this is an artifact of the linear regression estimating the mean instead of the mode: when you have *any* variance in the response up against a ceiling or floor (0 or 100), you should expect the mean to move away from that ceiling/floor. Thus there might be a bias in the *mean* response, but not necessarily the *mode* response.

We're also see something odd in `squarepie`, which was actually what I anticipated (and why I wrote this page): there are two faint lines in the error parallel to the central line. Theses lines are at exactly `+-10` from the center. The nature of a square pie graph is that it essentially decomposes into two bar graphs: a vertical one representing the 10s column of `p` (0, 10, 20, ...), and a horizontal bar graph stuck on top of it representing the ones column (0, 1, 2, 3, ...). Thus, say `p = 36` and someone misreads the tens column as `4`, they might respond with `45` or `46` or `47`. We can see this phenomenon more easily by looking at density plots of the error (the difference between people's responses and the actual probability depicted):

```{r}
sv %>%
    ggplot(aes(x = response - p)) +
    stat_density() +
    facet_wrap(~vis) +
    geom_vline(xintercept = 0, color="red") +
    coord_cartesian(xlim=c(-20,20))
```

Again we see that all of the conditions are basically unbiased. Note the bumps in density at +-10 for `squarepie`.

Let's see 1) if we can do a reasonable job of analyzing these first 


```{r}
sv %>%
    filter(vis == "squarepie") %>%
    filter(participant %in% paste0("p", 40:50)) %>%
    ggplot(aes(x = p, y=response)) +
    geom_point() +
    # geom_line(aes(color = participant)) +
    stat_smooth(aes(group=participant), method=lm, se=FALSE) +
    geom_abline(intercept = 0, slope=1)+
    geom_abline(intercept = 100, slope=-1)+
    facet_wrap(~participant)
```

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    filter(participant %in% paste0("p", 10:20)) %>%
    ggplot(aes(x = estimate-p)) +
    stat_density() + 
    geom_vline(xintercept = 0, linetype="dashed", color="red") +
    geom_vline(xintercept = -10, linetype="dashed", color="red") +
    geom_vline(xintercept = 10, linetype="dashed", color="red") 
    # facet_wrap(~participant)
```

And that is what we see. Let's see that as estimate versus value:

```{r}
sv %>%
    ggplot(aes(x = Value, y = Estimate)) +
    geom_point(alpha = 0.1) +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    facet_wrap(~Type)
```

And again we see the additional lines at +-10 for the square pie. I assume this happens when someone's estimate of the tens column (reading of the vertical "bar"" that is part of the square pie) is off by one, but their estimate of the ones column (reading of the horizontal "bar" that is part of the square pie) is still decent. 

Another thing worth noting: overall, people don't appear to be biased in any of the conditions: note how the estimates fall along the red line for `y = x`.

Let's fit a model to this (ignoring the off-by-ten errors in square pie for now):

```{r}
gen.trun(c(0, 1), family=NO, type="both")
gen.trun(c(0, 1), family=TF, type="both")
m = gamlss(Estimate ~ Value * Type, 
    sigma.formula = ~ Type + random(ID),
    data=sv, family=TF)
```

```{r}
transform_data = function(d) {
    capture.output({
        d = cbind(select(d, -Estimate), glimmer(Estimate ~ Value * Type + (1|ID), data=d)$d)
        d$ID = as.numeric(d$ID)
    })
    d
}

m = map(
    alist(
        Estimate ~ dnorm( mu , sigma ),
        mu <- Intercept +
            b_Value*Value +
            b_Typedonut*Typedonut +
            b_Typepiechart*Typepiechart +
            b_Typesquarepie*Typesquarepie +
            b_Value_X_Typedonut*Value_X_Typedonut +
            b_Value_X_Typepiechart*Value_X_Typepiechart +
            b_Value_X_Typesquarepie*Value_X_Typesquarepie,
        # +
            # v_Intercept[ID],
        Intercept ~ dnorm(0,10),
        b_Value ~ dnorm(0,10),
        b_Typedonut ~ dnorm(0,10),
        b_Typepiechart ~ dnorm(0,10),
        b_Typesquarepie ~ dnorm(0,10),
        b_Value_X_Typedonut ~ dnorm(0,10),
        b_Value_X_Typepiechart ~ dnorm(0,10),
        b_Value_X_Typesquarepie ~ dnorm(0,10),
        # v_Intercept[ID] ~ dnorm(0,sigma_ID),
        # sigma_ID ~ dcauchy(0,2),
        sigma ~ dcauchy(0,2)
    ),
    data = transform_data(sv)
)
```

```{r}
predictions = expand.grid(
        Value = 1:99,
        Type = factor(levels(sv$Type)),
        Estimate = 0,
        ID = factor("p01", levels = levels(sv$ID))
    ) %>%
    transform_data() %>%
    tidy_sim(m, predicted) %>%
    mean_qi(predicted) %>%
    tidy_link(m) %>%
    mean_qi(mu)
```

```{r}
sv %>%
    ggplot(aes(x = Value, y = Estimate)) +
    geom_point(alpha = 0.2) +
    geom_abline(intercept = 0, slope = 1, color="red") +
    geom_abline(intercept = 100, slope = -1, color="gray50", linetype= "dashed") +
    geom_ribbon(aes(ymin = predicted.lower, ymax = predicted.upper), data = predictions, alpha = 0.1) +
    geom_line(aes(y = mu), data = predictions) +
    facet_wrap(~ Type)
```

# Model for a single vis

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant : vector[n_participant]
        
        sigma_v_participant_sd : real(lower=0)
        
        log_nu : real
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        nu : real
        
        sigma_intercept ~ normal(0, 5)
        sigma_v_participant_sd ~ cauchy(0, 2)
        sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        mu <- mu_intercept + mu_b_p * (p - 50)

        log_nu ~ normal(0,1)
        nu <- exp(log_nu)
        
        # estimate - 50 ~ student_t(nu, mu, sigma)
        for (i in 1:n) {
            "(estimate[i] - 50) ~ student_t(nu, mu[i], sigma[i]) T[-50,50]"
        }
    },
    generated_quantities = {
        mu : vector[n]
        sigma : vector[n]
        nu : vector[n]

        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu <- mu_intercept + mu_b_p * (p - 50) + 50
        
        for (i in 1:n) {
            nu[i] <- exp(log_nu)
        }
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "bars")), algorithm="fullrank")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "bars")))
```


```{r}
rt.trun = trun.r(c(0, 100), family="TF", type="both")
qt.trun = trun.q(c(0, 100), family="TF", type="both")
pt.trun = function(x, ...) {
    (pTF(x, ...) - pTF(0, ...)) / (pTF(100, ...) - pTF(0, ...))
}
rnorm.trun = trun.r(c(0, 100), family="NO", type="both")

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    log_nu,
    sigma_v_participant_sd
    )[])

fitted = 
    tidy_samples(fit, c(mu, sigma, nu)[i]) %>%
    mutate(
        predicted = rt.trun(n(), mu, sigma, nu)
    ) %>%
    inner_join(mutate(filter(sv, vis=="bars"), i = 1:n())) %>%
    mutate(
        p_resid = pt.trun(estimate, mu, sigma, nu)
    )

residuals = fitted %>%
    group_by(i, p, estimate) %>%
    summarise(
        # p_resid = median(p_resid),
        p_resid = mean(estimate < predicted),
        z_resid = qnorm(p_resid),
        predicted = sample(predicted, 1),
        median_predicted = median(predicted),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        z_resid = z_resid / sd(z_resid),
        residual = predicted - estimate
    )
```

```{r}
residuals %>%
    flatworm(z_resid, p)
```

```{r}
mutate(sv, i = 1:n())
    
    group_by(i) %>%
    mean_qi()
    # mutate(
    #     estimate = rt.trun(n(), mu, sigma, 1)
    # ) %>%

predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            mutate(
                nu = exp(log_nu),
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * (p - 50) + 50,
                estimate = rt.trun(n(), mu, sigma, 1)
                # estimate = rnorm.trun(n(), mu, sigma)
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
sv %>%
    filter(vis == "bars") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=predicted - p), fill=NA, color="green", data=fitted)
```

```{r}
predictions %>%
    # select(-mu) %>% rename(mu = estimate) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = estimate), data=filter(sv, vis=="bars"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
    
```


# Model for square pie

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    transformed_data = {
        tens : vector[n]
        ones : vector[n]
        estimate_tens : vector[n]
        estimate_ones : vector[n]
        
        for (i in 1:n) {
            tens[i] = floor(p[i] / 10)
            ones[i] = fmod(p[i], 10)
            
            estimate_tens[i] = floor(estimate[i] / 10)
            estimate_ones[i] = fmod(estimate[i], 10)
        }
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        ones_sigma_intercept : real
        tens_sigma_intercept : real
        ones_sigma_v_participant : vector[n_participant]
        tens_sigma_v_participant : vector[n_participant]
        
        ones_sigma_v_participant_sd : real(lower=0)
        tens_sigma_v_participant_sd : real(lower=0)
        
        ones_log_nu : real
        tens_log_nu : real
    },
    model = {
        ones_mu : vector[n]
        tens_mu : vector[n]
        ones_sigma : vector[n]
        tens_sigma : vector[n]
        ones_nu : real
        tens_nu : real
        
        ones_sigma_intercept ~ normal(0, 5)
        tens_sigma_intercept ~ normal(0, 5)
        ones_sigma_v_participant_sd ~ cauchy(0, 2)
        ones_sigma_v_participant ~ normal(0, ones_sigma_v_participant_sd)
        tens_sigma_v_participant_sd ~ cauchy(0, 2)
        tens_sigma_v_participant ~ normal(0, tens_sigma_v_participant_sd)
        
        for (i in 1:n) {
            ones_sigma[i] <- exp(ones_sigma_intercept + ones_sigma_v_participant[participant[i]])
            tens_sigma[i] <- exp(tens_sigma_intercept + tens_sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        ones_mu <- mu_intercept + mu_b_p * (ones * 10 - 50)
        tens_mu <- mu_intercept + mu_b_p * (tens * 10 - 50)

        ones_log_nu ~ normal(5,2)
        tens_log_nu ~ normal(5,2)
        ones_nu <- exp(ones_log_nu)
        tens_nu <- exp(tens_log_nu)
        
        # estimate_ones * 10 - 50 ~ student_t(ones_nu, ones_mu, ones_sigma)
        # estimate_tens * 10 - 50 ~ student_t(tens_nu, tens_mu, tens_sigma)
        for (i in 1:n) {
            "(estimate_ones[i] * 10 - 50) ~ student_t(ones_nu, ones_mu[i], ones_sigma[i]) T[-50,50]"
            "(estimate_tens[i] * 10 - 50) ~ student_t(tens_nu, tens_mu[i], tens_sigma[i]) T[-50,50]"
        }
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="meanfield")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie")))
```


```{r}
samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    ones_sigma_intercept, tens_sigma_intercept, 
    ones_log_nu, tens_log_nu,
    ones_sigma_v_participant_sd, tens_sigma_v_participant_sd
    )[])

predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    mutate(
        ones = floor(p %% 10),
        tens = floor(p / 10)
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            mutate(
                p_tens = floor(p / 10) * 10,
                ones_nu = exp(ones_log_nu),
                tens_nu = exp(tens_log_nu),
                ones_sigma_v_participant = rnorm(n(), 0, ones_sigma_v_participant_sd),
                tens_sigma_v_participant = rnorm(n(), 0, tens_sigma_v_participant_sd),
                ones_sigma = exp(ones_sigma_intercept + ones_sigma_v_participant),
                tens_sigma = exp(tens_sigma_intercept + tens_sigma_v_participant),
                ones_mu = mu_intercept + mu_b_p * (ones * 10 - 50),
                tens_mu = mu_intercept + mu_b_p * (tens * 10 - 50),
                estimate_ones = (rTF(n(), ones_mu, ones_sigma, ones_nu) + 50) / 10,
                estimate_tens = round(rTF(n(), tens_mu, tens_sigma, ones_nu) / 10) * 10 + 50,
                estimate_ones = ifelse(estimate_ones > 9, 9, ifelse(estimate_ones < 0, 0, estimate_ones)),
                estimate_tens = ifelse(estimate_tens > p_tens + 10, p_tens + 10, ifelse(estimate_tens < p_tens -10, p_tens - 10, estimate_tens)),
                estimate = estimate_tens + estimate_ones
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
simplehist(predictions$error)
```

```{r}
sv %>%
    filter(vis == "squarepie", estimate - p > -20) %$%
    simplehist(estimate - p)
```


# Model for square pie based on p(off by one)

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant : vector[n_participant]
        
        offset_sigma_intercept : real

        sigma_v_participant_sd : real(lower=0)
        
        log_nu : real
        
        theta : simplex[3]
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        offset_sigma : vector[n]
        nu : real
        logps[3] : real    #temp for mixture log probabilities
        y : real
        mu_offset : real
        sigma_ : real

        
        sigma_intercept ~ normal(0, 5)
        sigma_v_participant_sd ~ cauchy(0, 2)
        sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        offset_sigma_intercept ~ normal(0, 5)

        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
            offset_sigma[i] <- exp(offset_sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        mu <- mu_intercept + mu_b_p * (p - 50)

        log_nu ~ normal(5,2)
        nu <- exp(log_nu)
        
        
        
        # estimate - 50 ~ student_t(nu, mu, sigma)
        theta[1] ~ beta(1,20)
        theta[2] ~ beta(20,1)
        theta[3] ~ beta(1,20)

        for (i in 1:n) {
            for (k in 1:3) {
                y = estimate[i] - 50
                mu_offset = (k * 10 - 20)
                sigma_ = sigma[i]
                if (k == 2) {
                    sigma_ = sigma[i]
                }
                else {
                    sigma_ = offset_sigma[i]
                }
                "logps[k] = log(theta[k]) + 
                    student_t_lpdf(y | nu, mu[i] + mu_offset, sigma_)"
                # +
                    # -log_diff_exp(
                        # normal_lcdf( 50 | mu[i] + mu_offset, sigma_),
                        # normal_lcdf(-50 | mu[i] + mu_offset, sigma_)
                    # )"
                # censoring at [-49,49], see Stan manual
                if (y == -49) { # estimate == 1
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lcdf(-49 | nu, mu[i] + mu_offset, sigma_)"
                    )
                }
                else if (y == 49) { #estimate == 99
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lccdf(49 | nu, mu[i] + mu_offset, sigma_)"
                    )
                }
            }
            "target += log_sum_exp(logps)"
        }
    },
    generated_quantities = {
        mu : vector[n]
        sigma : vector[n]
        k : vector[n]
        nu : vector[n]
        
        for (i in 1:n) {
            nu[i] <- exp(log_nu)
            k[i] <- categorical_rng(theta)
            if (k[i] == 2){
                sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
            }
            else {
                sigma[i] <- exp(offset_sigma_intercept + sigma_v_participant[participant[i]])
            }
        }

        mu <- mu_intercept + mu_b_p * (p - 50) + 50 + (k * 10 - 20)
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="fullrank")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie")))
```


```{r}
rt.trun = trun.r(c(0, 100), family="TF", type="both")

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    #log_nu,
    sigma_v_participant_sd
    )[]) %>%
    inner_join(
        tidy_samples(fit, theta[..]),
        by = ".sample"
    )


fitted = 
    tidy_samples(fit, c(mu, sigma, nu)[i]) %>%
    mutate(
        # predicted = rt.trun(n(), mu, sigma, 1)
        predicted = rTF(n(), mu, sigma, nu),
        predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
    ) %>%
    inner_join(mutate(filter(sv, vis=="squarepie"), i = 1:n())) %>%
    mutate(
        p_resid = pTF(estimate, mu, sigma, nu)
    )

residuals = fitted %>%
    group_by(i, p, estimate) %>%
    summarise(
        p_resid = median(p_resid),
        # p_resid = mean(estimate < predicted),
        z_resid = qnorm(p_resid),
        median_predicted = median(predicted),
        predicted = sample(predicted, 1),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        # z_resid = z_resid / sd(z_resid[is.finite(z_resid)]),
        residual = predicted - estimate
    )
```

```{r}
predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            group_by(.sample) %>%
            mutate(
                mu_offset = rcategorical(1, c(theta1, theta2, theta3)) * 10 - 20
            ) %>%
            ungroup() %>%
            mutate(
                # nu = exp(log_nu),
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * (p - 50) + 50 + mu_offset,
                estimate = rt.trun(n(), mu, sigma, 1)
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p), fill=NA, color="green", data=residuals)
```

```{r}
fitted10 = fitted %>%
    filter(.sample %in% 1:5)
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p, group = .sample), fill=NA, color="blue", data=fitted10)
```

```{r}
predictions %>%
    # select(-mu) %>% rename(mu = estimate) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = estimate), data=filter(sv, vis=="bars"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
    
```



# Model for square pie based on p(off by one 2) (student t)

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        response : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant : vector[n_participant]
        
        sigma_v_participant_sd : real(lower=0)
        
        log_nu : real
        
        theta : simplex[3]
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        nu : real
        logps[3] : real    #temp for mixture log probabilities
        y : real
        mu_offset : real
        theta_p : vector[3]

        
        sigma_intercept ~ normal(0, 5)
        sigma_v_participant_sd ~ cauchy(0, 2)
        sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        mu <- mu_intercept + mu_b_p * p

        log_nu ~ normal(5,2)
        nu <- exp(log_nu)
        
        theta_p[1] = 1
        theta_p[2] = 20
        theta_p[3] = 1
        theta ~ dirichlet(theta_p)

        for (i in 1:n) {
            for (k in 1:3) {
                mu_offset = (k * 10 - 20)

                "logps[k] = student_t_lpdf(response[i] | nu, mu[i] + mu_offset, sigma[i])"

                # censoring at [1, 100]
                if (response[i] == 1) {
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lcdf(1 | nu, mu[i] + mu_offset, sigma[i])"
                    )
                }
                else if (response[1] == 99) {
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lccdf(99 | nu, mu[i] + mu_offset, sigma[i])"
                    )
                }
                
                logps[k] = logps[k] + log(theta[k])
            }
            "target += log_sum_exp(logps)"
        }
    },
    generated_quantities = {
        mu : vector[n]
        sigma : vector[n]
        k : vector[n]
        nu : vector[n]
        
        for (i in 1:n) {
            nu[i] <- exp(log_nu)
            k[i] <- categorical_rng(theta)
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }

        mu <- mu_intercept + mu_b_p * p + (k * 10 - 20)
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="meanfield")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie")))
```


```{r}
rt.trun = trun.r(c(0, 100), family="TF", type="both")

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    log_nu,
    sigma_v_participant_sd
    )[]) %>%
    inner_join(
        tidy_samples(fit, theta[..]),
        by = ".sample"
    )


fitted = 
    tidy_samples(fit, c(mu, sigma, nu)[i]) %>%
    mutate(
        # predicted = rt.trun(n(), mu, sigma, 1)
        predicted = rTF(n(), mu, sigma, nu),
        # predicted = rlaplace(n(), mu, sigma),
        predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
    ) %>%
    inner_join(mutate(filter(sv, vis=="squarepie"), i = 1:n())) %>%
    mutate(
        p_resid = pTF(response, mu, sigma, nu)
        # p_resid = plaplace(response, mu, sigma)
    )

residuals = fitted %>%
    group_by(i, p, response, participant) %>%
    summarise(
        # p_resid = median(p_resid),
        p_resid = mean(response < predicted),
        z_resid = qnorm(p_resid),
        median_predicted = median(predicted),
        predicted = sample(predicted, 1),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        # z_resid = z_resid / sd(z_resid[is.finite(z_resid)]),
        residual = predicted - response
    )
```

```{r}
predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            filter(.sample %in% 1:1000) %>%
            group_by(.sample) %>%
            mutate(
                mu_offset = rcategorical(1, c(theta1, theta2, theta3)) * 10 - 20
            ) %>%
            ungroup() %>%
            mutate(
                nu = exp(log_nu),
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * p + mu_offset,
                predicted = rTF(n(), mu, sigma, nu),
                predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
            )
    }) %>%
    mutate(error = predicted - p)
```

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = response - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p), fill=NA, color="green", data=residuals)
```

```{r}
fitted10 = fitted %>%
    filter(.sample %in% 15:20)
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = response - p)) +
    stat_density(fill=NA, color="black", size=1.5) +
    stat_density(aes(x=round(predicted) - p, group = .sample), fill=NA, color="blue", data=fitted10) +
    coord_cartesian(xlim=c(-20,20))
```

```{r}
predictions %>%
    select(-mu) %>% rename(mu = predicted) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = response), data=filter(sv, vis=="squarepie"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
```

```{r}
predictions %>%
    ggplot(aes(x = p, y = predicted)) +
    geom_point(alpha=0.1)
```



# Model for square pie based on p(off by one 2) (laplace)

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        response : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real(lower=-6,upper=6)
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant_raw : vector[n_participant]
        
        sigma_v_participant_sd : real(lower=0)
        
        theta_k2 : real(lower=0, upper=1)
    },
    transformed_parameters = {
        sigma_v_participant : vector[n_participant]
        
        sigma_v_participant = sigma_v_participant_raw * sigma_v_participant_sd
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        logps[3] : real    #temp for mixture log probabilities
        y : real
        mu_offset : real
        # theta_p : vector[3]
        theta : vector[3]

        sigma_intercept ~ normal(1.5, 1.5)
        sigma_v_participant_sd ~ normal(0, 1.5)
        sigma_v_participant_raw ~ normal(0, 1) # => sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 0.5)
        mu_b_p ~ normal(1, 0.5)
        mu <- mu_intercept + mu_b_p * (p - 50) + 50

        # theta_p[1] = 1
        # theta_p[2] = 20
        # theta_p[3] = 1
        # theta ~ dirichlet(theta_p)
        # theta[1] ~ beta(1,20)
        # theta[2] ~ beta(20,1)
        # theta[3] ~ beta(1,20)
        theta_k2 ~ beta(20,1)
        theta[1] <- (1 - theta_k2) / 2
        theta[2] <- theta_k2
        theta[3] <- theta[1]

        for (i in 1:n) {
            for (k in 1:3) {
                mu_offset = k * 10 - 20

                # censoring at [1, 100]
                if (response[i] == 1) {
                    "logps[k] = student_t_lcdf(1.5 | 3, mu[i] + mu_offset, sigma[i])"
                }
                else if (response[i] == 99) {
                    "logps[k] = student_t_lccdf(98.5 | 3, mu[i] + mu_offset, sigma[i])"
                }
                else {
                    # "logps[k] = normal_lpdf(response[i] | mu[i] + mu_offset, sigma[i])"
                    #interval censoring
                    "logps[k] = log_diff_exp(
                        student_t_lcdf((response[i] + 0.5) | 3, mu[i] + mu_offset, sigma[i]),
                        student_t_lcdf((response[i] - 0.5) | 3, mu[i] + mu_offset, sigma[i])
                    )"
                }
                
                logps[k] = logps[k] + log(theta[k])
            }
            "target += log_sum_exp(logps)"
        }
    },
    generated_quantities = {
        mu : vector[n]
        mu_offset : vector[n]
        sigma : vector[n]
        k : vector[n]
        theta : vector[3]
        log_lik : vector[n]
        logps : vector[3]

        mu_offset_ : real

        theta[1] <- (1 - theta_k2) / 2
        theta[2] <- theta_k2
        theta[3] <- theta[1]

        for (i in 1:n) {
            k[i] <- categorical_rng(theta)
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_offset = k * 10 - 20
        mu <- mu_intercept + mu_b_p * (p - 50) + 50
        
        for (i in 1:n) {
            for (k_ in 1:3) {
                mu_offset_ = k_ * 10 - 20
                
                if (response[i] == 1) {
                    "logps[k_] = student_t_lcdf(1.5 | 3, mu[i] + mu_offset_, sigma[i])"
                }
                else if (response[i] == 99) {
                    "logps[k_] = student_t_lccdf(98.5 | 3, mu[i] + mu_offset_, sigma[i])"
                }
                else {
                    # "logps[k] = normal_lpdf(response[i] | mu[i] + mu_offset, sigma[i])"
                    #interval censoring
                    "logps[k_] = log_diff_exp(
                        student_t_lcdf((response[i] + 0.5) | 3, mu[i] + mu_offset_, sigma[i]),
                        student_t_lcdf((response[i] - 0.5) | 3, mu[i] + mu_offset_, sigma[i])
                    )"
                }
                # censoring at [1, 100]
                
                logps[k_] = logps[k_] + log(theta[k_])
            }
            log_lik[i] = log_sum_exp(logps)
        }
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit.v = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="fullrank")
```

```{r}
fit = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie"))
    # , iter = 8000
    # , thin = 4
    )
```

```{r}
summary(fit, pars=c("mu_intercept","mu_b_p","theta_k2","sigma_intercept","sigma_v_participant_sd"))
plot(fit, pars=c("mu_intercept","mu_b_p","theta_k2","sigma_intercept","sigma_v_participant_sd"))
```

```{r}
# dresid = dcauchy
# presid = pcauchy
# rresid = rcauchy
dresid = function(...) dTF(..., nu = 3)
presid = function(...) pTF(..., nu = 3)
rresid = function(...) rTF(..., nu = 3)

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    sigma_v_participant_sd
    )[]) %>%
    inner_join(
        tidy_samples(fit, theta[..]),
        by = ".sample"
    )


fitted = 
    tidy_samples(fit, c(mu, mu_offset, sigma)[i]) %>%
    mutate(
        predicted = rresid(n(), mu + mu_offset, sigma),
        predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
    ) %>%
    inner_join(mutate(filter(sv, vis=="squarepie"), i = 1:n())) %>%
    mutate(
        p_resid = presid(response, mu, sigma)
    )

residuals = fitted %>%
    group_by(i, p, response, participant) %>%
    summarise(
        # response_jitter = runif(1, response - 0.5, response + 0.5),
        # p_resid = median(p_resid),
        
        #randomized quantile residuals
        p_resid_max = mean(predicted < response + 0.5),
        p_resid_max = ifelse(p_resid_max == 0, 1/n(), p_resid_max),
        p_resid_min = mean(predicted < response - 0.5),
        p_resid_min = ifelse(p_resid_min == 1, (n() - 1)/n(), p_resid_min),
        p_resid = runif(1, p_resid_min, p_resid_max),
        z_resid = qnorm(p_resid),
        median_predicted = median(predicted),
        predicted = sample(predicted, 1),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        # z_resid = z_resid / sd(z_resid[is.finite(z_resid)]),
        residual = predicted - response
    )
```

```{r}
residuals %>% flatworm(z_resid, participant, ylim=NA) + facet_wrap(~.cuts)
```

```{r}
residuals %>% flatworm(z_resid)
```

```{r}
residuals %>% flatworm(z_resid, p)
```

```{r}
pairs(fit, pars=c("mu_intercept","mu_b_p","theta_k2","sigma_intercept","sigma_v_participant_sd"))
pairs(fit, pars=c("sigma_intercept","sigma_v_participant[1]","sigma_v_participant[2]","sigma_v_participant[3]"))
```

```{r}
predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            group_by(.sample) %>%
            mutate(
                mu_offset = rcategorical(1, c(theta1, theta2, theta3)) * 10 - 20
            ) %>%
            ungroup() %>%
            mutate(
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * (p - 50) + 50 + mu_offset,
                predicted = rresid(n(), mu, sigma),
                predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
            )
    }) %>%
    mutate(error = predicted - p)
```

```{r}
predictions %>%
    filter(p %in% (1:10 * 10)) %>%
    group_by(p) %>%
    mutate(
        mu = mu_intercept + mu_b_p * (p - 50) + 50,
        predicted = rresid(n(), mu, sigma)
    ) %>%
    summarise(
        above = mean(predicted > p + 2),
        below = mean(predicted < p - 2),
        theta2 = 1 - above - below
    )
```


```{r}
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = response - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p), fill=NA, color="green", data=residuals, adjust=1)
```

```{r}
fitted10 = fitted %>%
    filter(.sample %in% 15:20)
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = response - p)) +
    stat_density(fill=NA, color="black", size=1.5) +
    stat_density(aes(x=round(predicted) - p, group = .sample), fill=NA, color="blue", data=fitted10, adjust=1) +
    coord_cartesian(xlim=c(-20,20))
```

```{r}
predictions %>%
    select(-mu) %>% rename(mu = predicted) %>%
    # mutate(mu = mu - mu_offset) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = response), data=filter(sv, vis=="squarepie"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
```

```{r}
predictions %>%
    filter(.sample %in% 1:1000) %>%
    ggplot(aes(x = p, y = predicted)) +
    geom_point(alpha=0.1)
```

