---
title: "Reanalysis of BELIV Paper Study Data"
author: "Matthew Kay"
date: "July 18, 2016"
output: html_document
---

Based on `simpleviz-reanalysis.Rmd`.

# Setup

## Imports

```{r setup}
library(dplyr)
library(ggplot2)
library(metabayes)
library(tidybayes)
library(rstan)
library(rethinking)
import::from(gamlss, gamlss, random, re)
import::from(gamlss.dist, 
    NO, dNO, pNO, qNO, rNO, 
    TF, dTF, pTF, qTF, rTF)
import::from(gamlss.tr, gen.trun, trun.r)
```

## GGplot theme

```{r theme}
theme_set(theme_light())
```

## Stan setup

```{r stan}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

# Load and clean data

First, reading in the data and doing a bit of work to calculate error and reorder and rename the charts. We called the study _simplevis_ back then, so _sv_ is the prefix for the datasets.

```{r message=FALSE}
sv <- read.csv("data/simplevis.csv") %>%
    rename(
        p = Value,
        estimate = Estimate,
        participant = ID,
        vis = Type
    )
```


# Explore first

Let's see the shape of people's error. In particular, we're interested in Square Pie, which I suspect people are reading as two bar charts (one for the tens column and one for the ones column), so we should see two peaks at +-10 on either side of 0 for that chart.

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    filter(participant %in% paste0("p", 40:50)) %>%
    ggplot(aes(x = p, y=estimate)) +
    geom_point() +
    # geom_line(aes(color = participant)) +
    stat_smooth(aes(group=participant), method=lm, se=FALSE) +
    geom_abline(intercept = 0, slope=1)+
    geom_abline(intercept = 100, slope=-1)+
    facet_wrap(~participant)
```

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    filter(participant %in% paste0("p", 10:20)) %>%
    ggplot(aes(x = estimate-p)) +
    stat_density() + 
    geom_vline(xintercept = 0, linetype="dashed", color="red") +
    geom_vline(xintercept = -10, linetype="dashed", color="red") +
    geom_vline(xintercept = 10, linetype="dashed", color="red") 
    # facet_wrap(~participant)
```

And that is what we see. Let's see that as estimate versus value:

```{r}
sv %>%
    ggplot(aes(x = Value, y = Estimate)) +
    geom_point(alpha = 0.1) +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    facet_wrap(~Type)
```

And again we see the additional lines at +-10 for the square pie. I assume this happens when someone's estimate of the tens column (reading of the vertical "bar"" that is part of the square pie) is off by one, but their estimate of the ones column (reading of the horizontal "bar" that is part of the square pie) is still decent. 

Another thing worth noting: overall, people don't appear to be biased in any of the conditions: note how the estimates fall along the red line for `y = x`.

Let's fit a model to this (ignoring the off-by-ten errors in square pie for now):

```{r}
gen.trun(c(0, 1), family=NO, type="both")
gen.trun(c(0, 1), family=TF, type="both")
m = gamlss(Estimate ~ Value * Type, 
    sigma.formula = ~ Type + random(ID),
    data=sv, family=TF)
```

```{r}
transform_data = function(d) {
    capture.output({
        d = cbind(select(d, -Estimate), glimmer(Estimate ~ Value * Type + (1|ID), data=d)$d)
        d$ID = as.numeric(d$ID)
    })
    d
}

m = map(
    alist(
        Estimate ~ dnorm( mu , sigma ),
        mu <- Intercept +
            b_Value*Value +
            b_Typedonut*Typedonut +
            b_Typepiechart*Typepiechart +
            b_Typesquarepie*Typesquarepie +
            b_Value_X_Typedonut*Value_X_Typedonut +
            b_Value_X_Typepiechart*Value_X_Typepiechart +
            b_Value_X_Typesquarepie*Value_X_Typesquarepie,
        # +
            # v_Intercept[ID],
        Intercept ~ dnorm(0,10),
        b_Value ~ dnorm(0,10),
        b_Typedonut ~ dnorm(0,10),
        b_Typepiechart ~ dnorm(0,10),
        b_Typesquarepie ~ dnorm(0,10),
        b_Value_X_Typedonut ~ dnorm(0,10),
        b_Value_X_Typepiechart ~ dnorm(0,10),
        b_Value_X_Typesquarepie ~ dnorm(0,10),
        # v_Intercept[ID] ~ dnorm(0,sigma_ID),
        # sigma_ID ~ dcauchy(0,2),
        sigma ~ dcauchy(0,2)
    ),
    data = transform_data(sv)
)
```

```{r}
predictions = expand.grid(
        Value = 1:99,
        Type = factor(levels(sv$Type)),
        Estimate = 0,
        ID = factor("p01", levels = levels(sv$ID))
    ) %>%
    transform_data() %>%
    tidy_sim(m, predicted) %>%
    mean_qi(predicted) %>%
    tidy_link(m) %>%
    mean_qi(mu)
```

```{r}
sv %>%
    ggplot(aes(x = Value, y = Estimate)) +
    geom_point(alpha = 0.2) +
    geom_abline(intercept = 0, slope = 1, color="red") +
    geom_abline(intercept = 100, slope = -1, color="gray50", linetype= "dashed") +
    geom_ribbon(aes(ymin = predicted.lower, ymax = predicted.upper), data = predictions, alpha = 0.1) +
    geom_line(aes(y = mu), data = predictions) +
    facet_wrap(~ Type)
```

# Model for a single vis

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant : vector[n_participant]
        
        sigma_v_participant_sd : real(lower=0)
        
        log_nu : real
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        nu : real
        
        sigma_intercept ~ normal(0, 5)
        sigma_v_participant_sd ~ cauchy(0, 2)
        sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        mu <- mu_intercept + mu_b_p * (p - 50)

        log_nu ~ normal(0,1)
        nu <- exp(log_nu)
        
        # estimate - 50 ~ student_t(nu, mu, sigma)
        for (i in 1:n) {
            "(estimate[i] - 50) ~ student_t(nu, mu[i], sigma[i]) T[-50,50]"
        }
    },
    generated_quantities = {
        mu : vector[n]
        sigma : vector[n]
        nu : vector[n]

        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu <- mu_intercept + mu_b_p * (p - 50) + 50
        
        for (i in 1:n) {
            nu[i] <- exp(log_nu)
        }
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "bars")), algorithm="fullrank")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "bars")))
```


```{r}
rt.trun = trun.r(c(0, 100), family="TF", type="both")
qt.trun = trun.q(c(0, 100), family="TF", type="both")
pt.trun = function(x, ...) {
    (pTF(x, ...) - pTF(0, ...)) / (pTF(100, ...) - pTF(0, ...))
}
rnorm.trun = trun.r(c(0, 100), family="NO", type="both")

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    log_nu,
    sigma_v_participant_sd
    )[])

fitted = 
    tidy_samples(fit, c(mu, sigma, nu)[i]) %>%
    mutate(
        predicted = rt.trun(n(), mu, sigma, nu)
    ) %>%
    inner_join(mutate(filter(sv, vis=="bars"), i = 1:n())) %>%
    mutate(
        p_resid = pt.trun(estimate, mu, sigma, nu)
    )

residuals = fitted %>%
    group_by(i, p, estimate) %>%
    summarise(
        # p_resid = median(p_resid),
        p_resid = mean(estimate < predicted),
        z_resid = qnorm(p_resid),
        predicted = sample(predicted, 1),
        median_predicted = median(predicted),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        z_resid = z_resid / sd(z_resid),
        residual = predicted - estimate
    )
```

```{r}
residuals %>%
    flatworm(z_resid, p)
```

```{r}
mutate(sv, i = 1:n())
    
    group_by(i) %>%
    mean_qi()
    # mutate(
    #     estimate = rt.trun(n(), mu, sigma, 1)
    # ) %>%

predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            mutate(
                nu = exp(log_nu),
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * (p - 50) + 50,
                estimate = rt.trun(n(), mu, sigma, 1)
                # estimate = rnorm.trun(n(), mu, sigma)
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
sv %>%
    filter(vis == "bars") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=predicted - p), fill=NA, color="green", data=fitted)
```

```{r}
predictions %>%
    # select(-mu) %>% rename(mu = estimate) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = estimate), data=filter(sv, vis=="bars"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
    
```


# Model for square pie

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    transformed_data = {
        tens : vector[n]
        ones : vector[n]
        estimate_tens : vector[n]
        estimate_ones : vector[n]
        
        for (i in 1:n) {
            tens[i] = floor(p[i] / 10)
            ones[i] = fmod(p[i], 10)
            
            estimate_tens[i] = floor(estimate[i] / 10)
            estimate_ones[i] = fmod(estimate[i], 10)
        }
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        ones_sigma_intercept : real
        tens_sigma_intercept : real
        ones_sigma_v_participant : vector[n_participant]
        tens_sigma_v_participant : vector[n_participant]
        
        ones_sigma_v_participant_sd : real(lower=0)
        tens_sigma_v_participant_sd : real(lower=0)
        
        ones_log_nu : real
        tens_log_nu : real
    },
    model = {
        ones_mu : vector[n]
        tens_mu : vector[n]
        ones_sigma : vector[n]
        tens_sigma : vector[n]
        ones_nu : real
        tens_nu : real
        
        ones_sigma_intercept ~ normal(0, 5)
        tens_sigma_intercept ~ normal(0, 5)
        ones_sigma_v_participant_sd ~ cauchy(0, 2)
        ones_sigma_v_participant ~ normal(0, ones_sigma_v_participant_sd)
        tens_sigma_v_participant_sd ~ cauchy(0, 2)
        tens_sigma_v_participant ~ normal(0, tens_sigma_v_participant_sd)
        
        for (i in 1:n) {
            ones_sigma[i] <- exp(ones_sigma_intercept + ones_sigma_v_participant[participant[i]])
            tens_sigma[i] <- exp(tens_sigma_intercept + tens_sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        ones_mu <- mu_intercept + mu_b_p * (ones * 10 - 50)
        tens_mu <- mu_intercept + mu_b_p * (tens * 10 - 50)

        ones_log_nu ~ normal(5,2)
        tens_log_nu ~ normal(5,2)
        ones_nu <- exp(ones_log_nu)
        tens_nu <- exp(tens_log_nu)
        
        # estimate_ones * 10 - 50 ~ student_t(ones_nu, ones_mu, ones_sigma)
        # estimate_tens * 10 - 50 ~ student_t(tens_nu, tens_mu, tens_sigma)
        for (i in 1:n) {
            "(estimate_ones[i] * 10 - 50) ~ student_t(ones_nu, ones_mu[i], ones_sigma[i]) T[-50,50]"
            "(estimate_tens[i] * 10 - 50) ~ student_t(tens_nu, tens_mu[i], tens_sigma[i]) T[-50,50]"
        }
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="meanfield")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie")))
```


```{r}
samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    ones_sigma_intercept, tens_sigma_intercept, 
    ones_log_nu, tens_log_nu,
    ones_sigma_v_participant_sd, tens_sigma_v_participant_sd
    )[])

predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    mutate(
        ones = floor(p %% 10),
        tens = floor(p / 10)
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            mutate(
                p_tens = floor(p / 10) * 10,
                ones_nu = exp(ones_log_nu),
                tens_nu = exp(tens_log_nu),
                ones_sigma_v_participant = rnorm(n(), 0, ones_sigma_v_participant_sd),
                tens_sigma_v_participant = rnorm(n(), 0, tens_sigma_v_participant_sd),
                ones_sigma = exp(ones_sigma_intercept + ones_sigma_v_participant),
                tens_sigma = exp(tens_sigma_intercept + tens_sigma_v_participant),
                ones_mu = mu_intercept + mu_b_p * (ones * 10 - 50),
                tens_mu = mu_intercept + mu_b_p * (tens * 10 - 50),
                estimate_ones = (rTF(n(), ones_mu, ones_sigma, ones_nu) + 50) / 10,
                estimate_tens = round(rTF(n(), tens_mu, tens_sigma, ones_nu) / 10) * 10 + 50,
                estimate_ones = ifelse(estimate_ones > 9, 9, ifelse(estimate_ones < 0, 0, estimate_ones)),
                estimate_tens = ifelse(estimate_tens > p_tens + 10, p_tens + 10, ifelse(estimate_tens < p_tens -10, p_tens - 10, estimate_tens)),
                estimate = estimate_tens + estimate_ones
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
simplehist(predictions$error)
```

```{r}
sv %>%
    filter(vis == "squarepie", estimate - p > -20) %$%
    simplehist(estimate - p)
```


# Model for square pie based on p(off by one)

```{r}
m.meta = metastan(
    data = {
        n : int(lower=1)
        n_participant : int(lower=1)
        p : vector[n]
        estimate : vector[n]
        participant[n] : int
    },
    parameters = {
        mu_intercept : real
        mu_b_p : real
        
        sigma_intercept : real
        sigma_v_participant : vector[n_participant]
        
        offset_sigma_intercept : real

        sigma_v_participant_sd : real(lower=0)
        
        log_nu : real
        
        theta : simplex[3]
    },
    model = {
        mu : vector[n]
        sigma : vector[n]
        offset_sigma : vector[n]
        nu : real
        logps[3] : real    #temp for mixture log probabilities
        y : real
        mu_offset : real
        sigma_ : real

        
        sigma_intercept ~ normal(0, 5)
        sigma_v_participant_sd ~ cauchy(0, 2)
        sigma_v_participant ~ normal(0, sigma_v_participant_sd)
        
        offset_sigma_intercept ~ normal(0, 5)

        for (i in 1:n) {
            sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
            offset_sigma[i] <- exp(offset_sigma_intercept + sigma_v_participant[participant[i]])
        }
        
        mu_intercept ~ normal(0, 2)
        mu_b_p ~ normal(1, 2)
        mu <- mu_intercept + mu_b_p * (p - 50)

        log_nu ~ normal(5,2)
        nu <- exp(log_nu)
        
        
        
        # estimate - 50 ~ student_t(nu, mu, sigma)
        theta[1] ~ beta(1,20)
        theta[2] ~ beta(20,1)
        theta[3] ~ beta(1,20)

        for (i in 1:n) {
            for (k in 1:3) {
                y = estimate[i] - 50
                mu_offset = (k * 10 - 20)
                sigma_ = sigma[i]
                if (k == 2) {
                    sigma_ = sigma[i]
                }
                else {
                    sigma_ = offset_sigma[i]
                }
                "logps[k] = log(theta[k]) + 
                    student_t_lpdf(y | nu, mu[i] + mu_offset, sigma_)"
                # +
                    # -log_diff_exp(
                        # normal_lcdf( 50 | mu[i] + mu_offset, sigma_),
                        # normal_lcdf(-50 | mu[i] + mu_offset, sigma_)
                    # )"
                # censoring at [-49,49], see Stan manual
                if (y == -49) { # estimate == 1
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lcdf(-49 | nu, mu[i] + mu_offset, sigma_)"
                    )
                }
                else if (y == 49) { #estimate == 99
                    logps[k] = log_sum_exp(logps[k],
                        "student_t_lccdf(49 | nu, mu[i] + mu_offset, sigma_)"
                    )
                }
            }
            "target += log_sum_exp(logps)"
        }
    },
    generated_quantities = {
        mu : vector[n]
        sigma : vector[n]
        k : vector[n]
        nu : vector[n]
        
        for (i in 1:n) {
            nu[i] <- exp(log_nu)
            k[i] <- categorical_rng(theta)
            if (k[i] == 2){
                sigma[i] <- exp(sigma_intercept + sigma_v_participant[participant[i]])
            }
            else {
                sigma[i] <- exp(offset_sigma_intercept + sigma_v_participant[participant[i]])
            }
        }

        mu <- mu_intercept + mu_b_p * (p - 50) + 50 + (k * 10 - 20)
    }
)
m.stan = stan_model(model_code = as.character(m.meta))
```

```{r}
fit = vb(m.stan, data = compose_data(filter(sv, vis == "squarepie")), algorithm="fullrank")
```

```{r}
fit.s = sampling(m.stan, data = compose_data(filter(sv, vis == "squarepie")))
```


```{r}
rt.trun = trun.r(c(0, 100), family="TF", type="both")

samples = tidy_samples(fit, c(mu_intercept, mu_b_p, 
    sigma_intercept,
    #log_nu,
    sigma_v_participant_sd
    )[]) %>%
    inner_join(
        tidy_samples(fit, theta[..]),
        by = ".sample"
    )


fitted = 
    tidy_samples(fit, c(mu, sigma, nu)[i]) %>%
    mutate(
        # predicted = rt.trun(n(), mu, sigma, 1)
        predicted = rTF(n(), mu, sigma, nu),
        predicted = ifelse(predicted < 1, 1, ifelse(predicted > 99, 99, predicted))
    ) %>%
    inner_join(mutate(filter(sv, vis=="squarepie"), i = 1:n())) %>%
    mutate(
        p_resid = pTF(estimate, mu, sigma, nu)
    )

residuals = fitted %>%
    group_by(i, p, estimate) %>%
    summarise(
        p_resid = median(p_resid),
        # p_resid = mean(estimate < predicted),
        z_resid = qnorm(p_resid),
        median_predicted = median(predicted),
        predicted = sample(predicted, 1),
        mu = mean(mu)
    ) %>%
    ungroup() %>%
    mutate(
        # z_resid = z_resid / sd(z_resid[is.finite(z_resid)]),
        residual = predicted - estimate
    )
```

```{r}
predictions = 
    expand.grid(
        p = 1:99
    ) %>%
    group_by(p) %>%
    do({
        cbind(., samples) %>%
            group_by(.sample) %>%
            mutate(
                mu_offset = rcategorical(1, c(theta1, theta2, theta3)) * 10 - 20
            ) %>%
            ungroup() %>%
            mutate(
                # nu = exp(log_nu),
                sigma_v_participant = rnorm(n(), 0, sigma_v_participant_sd),
                sigma = exp(sigma_intercept + sigma_v_participant),
                mu = mu_intercept + mu_b_p * (p - 50) + 50 + mu_offset,
                estimate = rt.trun(n(), mu, sigma, 1)
            )
    }) %>%
    mutate(error = estimate - p)
```

```{r}
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p), fill=NA, color="green", data=residuals)
```

```{r}
fitted10 = fitted %>%
    filter(.sample %in% 1:5)
sv %>%
    filter(vis == "squarepie") %>%
    ggplot(aes(x = estimate - p)) +
    stat_density(fill=NA, color="red") +
    stat_density(aes(x=round(predicted) - p, group = .sample), fill=NA, color="blue", data=fitted10)
```

```{r}
predictions %>%
    # select(-mu) %>% rename(mu = estimate) %>%
    mean_qi(mu) %>%
    ggplot(aes(x = p, y = mu)) +
    geom_point(aes(y = estimate), data=filter(sv, vis=="bars"), alpha=0.1) +
    geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.25) +
    geom_abline(intercept = 0, slope = 1, color="gray50") +
    geom_line(color="red")
    
```

